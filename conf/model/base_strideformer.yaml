first_model_name_or_path: "sentence-transformers/all-MiniLM-L12-v2"
freeze_first_model: yes
max_chunks: 128
hidden_size: 768
initializer_range: 0.02
num_hidden_layers: 36
num_attention_heads: 12
intermediate_size: 4096
hidden_act: "gelu"
dropout: 0.1
layer_norm_eps: 1e-7
num_labels: 6