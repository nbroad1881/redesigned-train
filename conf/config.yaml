# Default configuration

defaults:
  - data: default
  - training_args: default
  - model: base_transformer
  - wandb: default
  - _self_ # attributes in this file will override defaults


model:
  model_name_or_path:  "roberta-large"
  short_model_max_chunks: 128
  hidden_act: gelu
  intermediate_size: 4096
  layer_norm_eps: 1e-7
  num_attention_heads: 12
  num_hidden_layers: 36
  multisample_dropout: []
  loss_fn: mse # smoothl1, l1, mse, ce

  
data:
  max_seq_length: 512
  pad_multiple: 8
  stride: null
  n_rows: -1
  map_batch_size: 500 # batch size when using `datasets.map`
  dataset_name: null
  data_files:
    train:
      - 'train_folds.csv'
  mask_augmentation: no
  problem_type: "multi_label_classification"
  
training_args:
  do_train: yes
  do_eval: yes
  
  fp16: yes
  
  learning_rate: 9e-6
  per_device_train_batch_size: 8
  per_device_eval_batch_size: 48
  gradient_accumulation_steps: 1
  
  metric_for_best_model: "eval_mcrmse"
  greater_is_better: no
  report_to: "wandb"
  log_level: "warning"
  save_strategy: "no"
  evaluation_strategy: "steps"
  eval_steps: 150
  eval_delay: 200
  logging_steps: 30
  save_total_limit: 2
  group_by_length: yes
  warmup_ratio: 0.1
  optim: adamw_bnb_8bit
  
wandb:
  group: "mlm-tests"
  tags:
    - "pass_config_to_model"
    - "clamp scores 1-5"
  
# general configuration
num_proc: 7
task: text-classification
language: en
project_name: feedback-prize-3
run_start: ""
fold: -1

# if the eval score is better than this, save
threshold_score: 0.5 
# typically do all 5, but for quick experimentation, 2 or 3 are good enough[
folds_to_run: 3

hydra:
  output_subdir: config