# Default configuration

defaults:
  - data: default
  - training_args: default
  - model: default
  - wandb: default
  - _self_ # attributes in this file will override defaults


model:
  model_name_or_path:  "roberta-base"
  short_model_max_chunks: 128
  hidden_act: gelu
  intermediate_size: 4096
  layer_norm_eps: 1e-7
  num_attention_heads: 12
  num_hidden_layers: 36

  
data:
  max_seq_length: 512
  pad_multiple: 8
  stride: None
  n_rows: 200
  map_batch_size: 500 # batch size when using `datasets.map`
  dataset_name: null
  data_files:
    train:
      - 'train_folds.csv'
    validation:
  
training_arguments:
  do_train: yes
  do_eval: yes
  
  evaluation_strategy: "epoch"
  fp16: yes
  
  learning_rate: 7e-6
  per_device_train_batch_size: 2
  per_device_eval_batch_size: 16
  gradient_accumulation_steps: 1
  
  metric_for_best_model: "eval_mcrmse"
  greater_is_better: no
  report_to: "wandb"
  log_level: "warning"
  save_strategy: "epoch"
  logging_steps: 10
  save_total_limit: 2
  
# general configuration
num_proc: 2
task: text-classification
language: en
project_name: feedback-prize-3

hydra:
  output_subdir: config